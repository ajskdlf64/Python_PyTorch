{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Fashion MNIIST DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에서 어쩌면 모델보다 중요한 것이 바로 좋은 데이터셋이다. 영미권에서도 \"콩 심은 데 콩 나고 팥 심은 데 팥 난다.\" 라는 뜻인 \"Garbage In Garbage Out\" 이라는 표현을 많이 사용한다. 데이터셋은 우리가 풀고자 하는 문제를 정의한다. 문제 정의가 잘못되면 아무리 풀이를 잘해도 별 의미가 없기 때문에 데이터셋을 관찰하고 관리하는 기술도 매우 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에 입문하면서 흔히 만나는 데이터셋이 바로 MNIST라는 손글시 숫자 데이터셋이다. 숫자를 인식하는 것은 이제 식상하다고 생각하므로, 패션 아이템을 모아놓은 데이터셋인 **Fashion MNIST** 를 다뤄보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fashion MNIST** 는 $28 \\times 28$ 픽셀 70,000개의 흑백 이미지로 구성되며 신발, 드레스, 가방 등 총 10가지 카테고리가 존재한다. 다음 그림은 이 데이터셋 안의 이미지를 무작위로 모아본 모습이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![패션 mnist](./Image/figure16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 모델을 만들고 학습하기에 앞서 데이터를 모아서 가공하는 재미없는 일부터 시작해야 한다. 실제 서비스를 만들 때, 엔지니어 시간의 대부분은 데이터를 가공하고 파이프라인을 만드는 데 쓰인다. 다행히 **토치비전** 이 Fashion MNIST 데이터셋을 자동으로 내려받고 학습 데이터를 나누는 일까지 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 토치비전과 맷플로립을 이용하여 데이터셋을 관찰해보자. 데이터를 관찰할 때는 주피터 노트북이 편리하므로 주피터 노트북을 이용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지 데이터를 다루기 위한 파이토치와 토치비전의 몇 가지 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. torch.utils.data** : 데이터셋의 표준을 정의하고 데이터셋을 불러오고 자르고 섞는 데 쓰는 도구들이 들어있는 모듈이다. 파이토치 모델을 학습시키기 위한 데이터셋의 표준을 **torch.utils.data.Dataset** 에 정의한다. Dataset 모듈을 상속하는 파생 클래스는 학습에 필요한 데이터를 로딩해주는 **torch.utils.data.DataLoader** 인스턴스의 입력으로 사용할 수 있다.\n",
    "\n",
    "**2. torchvision.datasets** : torch.utils.data.Dataset을 상속하는 이미지 데이터셋의 모음이다. Fashion MNIST 데이터셋이 여기에 들어 있다.\n",
    "\n",
    "\n",
    "**3. torchvision.transfoms** : 이미지 데이터셋에 쓸 수 있는 여러 가지 변환 필터를 담고 있는 모듈이다. 예를 들어 텐서로 변환한다든지, 크기조절(resize), 크롭(crop)으로 이미지를 수정할 수 있고, 밝기(brightness), 대비(contrast) 등을 조절하는 데 사용될 수 있다.\n",
    "\n",
    "**4. torchvision.utils** : 이미지 데이터를 저장하고 시각화하기 위한 도구가 들어있는 모듈이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import utils\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 이미지를 텐서로 바꿔보자. 토치비전의 transforms는 입력을 변환시키는 도구이다. ToTensor() 함수를 이용해 텐서로 바꾸거나, Resize()를 이용해 크기를 조정하거나, Normalize()를 이용해 정규화를 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 텐서로 변환\n",
    "transform = transforms.Compose([transforms.ToTensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는 FashionMNIST 데이터셋을 가져올 차례이다. 토치비전의 datasets 패키지는 데이터셋을 내려받고, Compose로 만들어 둔 이미지 변환 설정을 적용하는 데 쓰인다. download=True 옵션을 넣고, 현재 \"root\"로 지정한 폴더에 데이터셋이 존재하는지 확인한 후, 없으면 자동으로 저장된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 FashionMNIST 데이터셋은 학습용 트레이닝셋과 성능 평가용 테스트셋으로 나뉘어 있다. 어느 것을 받을지는 \"train\" 매개변수에 \"True\"나 \"False\"를 주어 선택할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train Set\n",
    "trainset = datasets.FashionMNIST(root = \"./data/\",\n",
    "                                 train = True,\n",
    "                                 download = True,\n",
    "                                 transform = transform)\n",
    "\n",
    "# Test Set\n",
    "testset = datasets.FashionMNIST(root = \"./data/\",\n",
    "                                train = False,\n",
    "                                download = True,\n",
    "                                transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvision.datasets로 생성된 객체는 파이토치 내부 클래스 \"torch.utils.data.Dataset\"을 상속한다. 그러므로 파이토치의 \"DataLoader\", 즉 데이터셋을 로딩하는 클래스에 넣어 바로 사용할 수 있다. \"DataLoader\"는 데이터셋을 배치(batch)라는 작은 단위로 쪼개고 학습 시 반복문 안에서 데이터를 공급해주는 클래스이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기는 한 번에 처리하는 개수를 뜻한다. 예를 들어 다음과 같이 배치 크기(batch_size)가 16이면 반복마다 이미지를 16개씩 읽어준다. 컴퓨터 메모리 공간에 여유가 있다면 더 크게 해도 되고, 여유가 없다면 적게 해도 상관없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터로더(DataLoader)의 매개변수에 앞서 불러온 데이터셋을 넣어주고 배치 크기를 지정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = data.DataLoader(dataset = trainset,\n",
    "                               batch_size = batch_size)\n",
    "\n",
    "test_loader = data.DataLoader(dataset = testset,\n",
    "                              batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터로더가 준비되었으므로 이제부터는 편리하게 데이터를 뽑아 쓸 수 있다. 보통은 for 문 안에 넣어서 사용하면 되지만, 배치 1개만 뽑아 데이터가 어떻게 생겼는제 보자. iter() 함수를 이용하여 반복문 안에서 이용할 수 있도록 만들어준 다음 next() 함수를 이용하여 배치 1개를 가져온다. 배치 1개의 images와 labels에는 앞서 설정한 배치 크기만큼 각각 이미지 16개와 레이블 16개가 들어 있게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object() takes no parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d0bf0ef79516>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataiter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object() takes no parameters"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 숲을 먼저 보고 나무를 보자. 앞서 가져온 배치의 이미지들을 전체적으로 살펴보면 어떤 느낌인지 알 수 있다. 토치비전의 utilsmake_grid() 함수를 이용하면 여러 이미지를 모아 하나의 이미지로 만들 수 있다. 이 때 img는 파이토치 텐서이기 때문에 numpy() 함수로 matplotlib과 호환이 되는 넘파이 행렬로 바꿔준다. 그리고 matplotlib이 인식하는 차원의 순서가 다르므로, np.transpose() 함수를 이용해 첫 번째(0번째) 차원을 맨 뒤로 보낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure() 함수로 이미지가 들어갈 자리를 만들고 plt.imshow() 함수와 plt.show() 함수를 순서대로 부르면 주피터노트북에서 이미지를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = utils.make_grid(images, padding = 0)\n",
    "npimg = img.numpy()\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 보는것 처럼 여러 개의 패션 아이템이 나열되는 것을 볼 수 있다. 화질이 좋지 않고 흑백이지만 프로토타입용 예제로는 좋은 데이터셋이다. 단순한 학습 데이터로 먼저 빠르게 인공 신경망을 만들어 실험해볼 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블을 출력해보면 각각의 이미지가 뜻하는 숫자 16개가 나열된 것을 볼 수 있다. 이미지들의 클래스들은 \"티셔츠/윗옷\", \"바지\", \"스웨터\", \"드레스\", \"코트\", \"샌들\", \"셔츠\", \"운동화\", \"가방\", \"앵클부츠\", 이렇게 10가지가 나온다. 앞엣 보았듯이 데이터셋에서는 이름 대신에 숫자 번호로 레이블이 주어진다. 다음과 같이 한글로 편하게 딕셔너리를 만들어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = {0 : \"T-shirt/Top\",\n",
    "           1 : \"Trouser\",\n",
    "           2 : \"Pullover\",\n",
    "           3 : \"Dress\",\n",
    "           4 : \"Coat\",\n",
    "           5 : \"Sandal\",\n",
    "           6 : \"Shirt\",\n",
    "           7 : \"Sneaker\",\n",
    "           8 : \"Bag\",\n",
    "           9 : \"Ankle boot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바뀐 딕셔너리\n",
    "for label in labels : \n",
    "    index = label.item()\n",
    "    print(CLASSES[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지의 데이터는 가로, 세로, 색상으로 구성된 3차원 행렬로 표현된다. 가로세로는 각각의 픽셀 수, 그리고 색상값은 흑백사진이니 1가지 숫자로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![패션 mnist](./Image/figure17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 픽셀은 0부터 255까지 값을 갖는다. Fashion MNIST에서 이미지의 크기는 $28 \\times 28$, 색상 채널은 흑백 1가지이다. 그러므로 입력 $x$의 특징값 총개수는 $ 28 \\times 28 \\times 1$개, 즉 784개이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개별 이미지를 꺼내 보자. Fashion MNIST 데이터셋에서 첫 번째 이미지를 꺼내고, squeeze()와 numpy() 함수를 적용하여 맷플롯립에서 이용이 가능한 넘파이 행렬을 만든다. 그리고 앞서 만들어둔 딕셔너리를 이용하여 레이블 번호를 사람이 알아볼 수 있는 문자로 바꾼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "item_img = images[idx]\n",
    "item_npimg = item_img.squeeze().numpy()\n",
    "plt.title(CLASS[labels[idx].item()])\n",
    "plt.imshow(item_npimg, cmap=[\"gray\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 인공 신경망으로 패션 아이템 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 Fashion MNIST 데이터셋에 대해 살펴보자. 이번 예제에서는 패션 아이템 이미지를 인식하여 레이블을 예측하는 기본적인 심층 인공 신경망(deep neural network, DNN)을 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 환경 설정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 파이토치, 인공 신경망 모델의 재료들을 담고 있는 nn 모듈, 최적화를 위한 optim 모듈, nn 모듈의 함수 버전인 functional을 F로 불러온다. 그 다음 토치비전의 데이터셋을 다루기 위해 transforms와 datasets를 임포트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.cuda.is_available() 함수는 현재 컴퓨터에서 CUDA를 이용할수 있는지 알아보는 함수이다. CUDA용 PyTorch를 설치하고 CUDA도 제대로 설치했다면 True를 반환하고, CUDA를 설치하지 않았거나 오류가 있다면 False를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 값을 기준으로 CUDA를 지원하면 \"cuda\"를, 아니면 \"cpu\"를 torch.device에 설정한 후 변수에 저장해놓으면 여러 환경에서 돌아가야 하는 코드를 공유해야할 때 유용하게 쓸 수 있다. 이번에는 이 문자열을 DEVICE라는 변수에 저장해두자. 이 변수는 나중에 텐서와 가중치에 대한 연산을 CPU와 GPU를 가진 누군가가 돌려볼 수 있도록 항상 이 코드를 포함하는 것을 추천한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"현재 PyTorch 환경 : \" + str(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion MNIST는 데이터가 많다. 이미지가 수만 개이므로 전체 데이터를 한꺼번에 사용하는 것은 효율적이지 않으니, 여러 개의 배치로 잘라 사용할 것이다. 각 미니배치의 크기는 64개로 하자. 에폭은 학습 데이터를 총 몇 번 반복하는지에 대한 설정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 이미지 분류 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 분류(Image Classification)는 한 장의 이미지를 받아 이 이미지가 어느 클래스에 속하는지 알려주는 문제이다. 상대적으로 단순한 문제이지만 상업적으로 가장 영향력이 있는 문제이기도 하다. 인공 신경망을 이용한 이미지 분류 알고리즘들은 이미 여러 서비스에서 이미지 기반 검색, 추천, 광고 등에 사용되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![패션 mnist](./Image/figure18.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연속된 이미지라고 할 수 있는 비디오 분류 모델들도 결국 기본적인 이미지 분류 방식에서 크게 벗어나지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 이미지 분류를 위한 인공 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
